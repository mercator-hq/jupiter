# Example Provider Configuration
# This file demonstrates how to configure different LLM providers

providers:
  # OpenAI Configuration
  openai:
    type: "openai"
    base_url: "https://api.openai.com/v1"
    api_key: "${OPENAI_API_KEY}"  # Load from environment variable
    timeout: "60s"
    max_retries: 3
    health_check_interval: "30s"
    connection_pool:
      max_idle_conns: 100
      max_idle_conns_per_host: 10
      idle_timeout: "90s"

  # Anthropic Configuration
  anthropic:
    type: "anthropic"
    base_url: "https://api.anthropic.com"
    api_key: "${ANTHROPIC_API_KEY}"  # Load from environment variable
    timeout: "60s"
    max_retries: 3
    health_check_interval: "30s"
    connection_pool:
      max_idle_conns: 100
      max_idle_conns_per_host: 10
      idle_timeout: "90s"

  # Ollama (Local) Configuration
  ollama:
    type: "generic"
    base_url: "http://localhost:11434/v1"
    api_key: ""  # No API key needed for local Ollama
    timeout: "120s"  # Longer timeout for local inference
    max_retries: 1   # Don't retry local requests
    health_check_interval: "10s"  # Check more frequently
    connection_pool:
      max_idle_conns: 10
      max_idle_conns_per_host: 5
      idle_timeout: "60s"

  # LM Studio (Local) Configuration
  lmstudio:
    type: "generic"
    base_url: "http://localhost:1234/v1"
    api_key: ""  # No API key needed for LM Studio
    timeout: "120s"
    max_retries: 1
    health_check_interval: "10s"
    connection_pool:
      max_idle_conns: 10
      max_idle_conns_per_host: 5
      idle_timeout: "60s"

  # vLLM (Self-hosted) Configuration
  vllm:
    type: "generic"
    base_url: "http://localhost:8000/v1"
    api_key: ""  # Set if authentication is enabled
    timeout: "90s"
    max_retries: 2
    health_check_interval: "30s"
    connection_pool:
      max_idle_conns: 50
      max_idle_conns_per_host: 10
      idle_timeout: "90s"

  # Custom OpenAI-Compatible Provider
  custom-llm:
    type: "generic"
    base_url: "https://my-custom-llm.example.com/v1"
    api_key: "${CUSTOM_LLM_KEY}"
    timeout: "60s"
    max_retries: 3
    health_check_interval: "30s"
    connection_pool:
      max_idle_conns: 100
      max_idle_conns_per_host: 10
      idle_timeout: "90s"

# Configuration Guidelines:
#
# Provider Types:
#   - "openai": For OpenAI API
#   - "anthropic": For Anthropic Messages API
#   - "generic": For any OpenAI-compatible API
#
# Timeout Guidelines:
#   - Cloud providers: 30-60 seconds
#   - Local models: 90-120 seconds (inference can be slow)
#   - GPU-accelerated: 30-60 seconds
#
# Retry Guidelines:
#   - Cloud providers: 3 retries (transient errors common)
#   - Local providers: 1 retry (no point retrying local failures)
#
# Health Check Interval:
#   - Cloud providers: 30-60 seconds
#   - Local providers: 10-30 seconds (detect failures faster)
#
# Connection Pool Guidelines:
#   - High-traffic cloud providers: 100 max idle, 10 per host
#   - Low-traffic or local: 10 max idle, 5 per host
#   - Idle timeout: 60-90 seconds
#
# Environment Variables:
#   Use ${VAR_NAME} syntax to load from environment variables.
#   This is the recommended way to handle API keys.
#
# Required Fields:
#   - name: Provider identifier (must be unique)
#   - type: Provider type (openai, anthropic, generic)
#   - base_url: API endpoint base URL
#   - api_key: Authentication key (optional for local providers)
